[
  {
    "id": "code_1",
    "type": "code",
    "section": "五、代码题",
    "title": "编写脚本监控系统CPU和内存使用率，超过阈值时发送警告。",
    "code": "#!/bin/bash\n#定义阈值\nCPU_THRESHOLD=80\nMEMORY_THRESHOLD=90\n\n# 获取CPU使用率（去掉百分号）\nCPU_USAGE=$(top -bn1 | grep \"Cpu(s)\" | awk '{print 100 - $8}' | cut -d'.' -f1)\n\n# 获取内存使用率\nMEMORY_USAGE=$(free | awk '/Mem/ {printf \"%.0f\", $3/$2 * 100}')\n\n# 检查CPU使用率\nif [ $CPU_USAGE -gt $CPU_THRESHOLD ]; then\n  echo \"警告: CPU使用率 ${CPU_USAGE}% 超过阈值 ${CPU_THRESHOLD}%\" | mail -s \"系统监控警告\" admin@example.com\nfi\n\n# 检查内存使用率\nif [ $MEMORY_USAGE -gt $MEMORY_THRESHOLD ]; then\n  echo \"警告: 内存使用率 ${MEMORY_USAGE}% 超过阈值 ${MEMORY_THRESHOLD}%\" | mail -s \"系统监控警告\" admin@example.com\nfi\n\necho \"监控完成: CPU=${CPU_USAGE}%, 内存=${MEMORY_USAGE}%\"",
    "language": "bash",
    "note": "请查资料补全代码注释"
  },
  {
    "id": "code_2",
    "type": "code",
    "section": "五、代码题",
    "title": "使用生成器实现斐波那契数列，并演示惰性求值优势。",
    "code": "def fibonacci_generator():\n    \"\"\"生成器实现斐波那契数列\"\"\"\n    a, b = 0, 1\n    while True:\n        yield a\n        a, b = b, a + b\n\ndef demo_lazy_evaluation():\n    # 创建生成器\n    fib_gen = fibonacci_generator()\n    # 只获取前20个斐波那契数，不会计算整个无限序列\n    first_20 = [next(fib_gen) for _ in range(20)]\n    print(\"前20个斐波那契数:\", first_20)\n\n    # 内存效率演示：生成器不存储所有值\n    import sys\n    fib_gen2 = fibonacci_generator()\n    size_small = sys.getsizeof(fib_gen2)\n\n    # 对比列表存储所有值的内存占用\n    fib_gen3 = fibonacci_generator()\n    fib_list = [next(fib_gen3) for _ in range(1000)]\n    size_large = sys.getsizeof(fib_list)\n\n    print(f\"生成器内存占用: {size_small} bytes\")\n    print(f\"列表内存占用: {size_large} bytes\")\n    print(f\"内存节省: {size_large - size_small} bytes\")\n\nif __name__ == \"__main__\":\n    demo_lazy_evaluation()",
    "language": "python",
    "note": "请查资料补全代码注释"
  },
  {
    "id": "code_3",
    "type": "code",
    "section": "五、代码题",
    "title": "使用Hugging Face Transformers库实现文本摘要任务。",
    "code": "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\nclass TextSummarizer:\n    def __init__(self, model_name=\"facebook/bart-large-cnn\"):\n        \"\"\"初始化文本摘要器\"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n        # 使用 pipeline 简化操作\n        self.summarizer = pipeline(\n            \"summarization\",\n            model=self.model,\n            tokenizer=self.tokenizer\n        )\n\n    def summarize(self, text, max_length=150, min_length=50):\n        \"\"\"生成文本摘要\"\"\"\n\n        # 预处理文本\n        if len(text.split()) < 50:\n            return \"文本过短，无法生成有效摘要\"\n\n        # 生成摘要\n        summary = self.summarizer(\n            text,\n            max_length=max_length,\n            min_length=min_length,\n            do_sample=False\n        )\n        return summary[0]['summary_text']\n\n    def evaluate_summary(self, original_text, summary):\n        \"\"\"评估摘要质量\"\"\"\n        \n        # 计算压缩比\n        orig_words = len(original_text.split())\n        summ_words = len(summary.split())\n        compression_ratio = (orig_words - summ_words) / orig_words * 100\n\n        print(f\"原文长度: {orig_words} 词\")\n        print(f\"摘要长度: {summ_words} 词\")\n        print(f\"压缩率: {compression_ratio:.1f}%\")\n\n        # 简单的内容保留评估\n        important_words = set(original_text.lower().split()[:10])\n        preserved_words = important_words.intersection(set(summary.lower().split()))\n        preservation_rate = len(preserved_words) / len(important_words) * 100\n\n        print(f\"重要词保留率: {preservation_rate:.1f}%\")\n\n# 使用示例\nif __name__ == \"__main__\":\n    # 初始化摘要器\n    summarizer = TextSummarizer()\n\n    # 示例文本\n    sample_text = \"\"\"\n    人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器。\n    该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。\n    \"\"\"\n\n    # 生成摘要\n    summary = summarizer.summarize(sample_text)\n    print(\"生成的摘要:\", summary)\n\n    # 评估摘要质量\n    print(\"\\n\" + \"=\" * 50)\n    summarizer.evaluate_summary(sample_text, summary)",
    "language": "python",
    "note": "请查资料补全代码注释"
  },
  {
    "id": "code_4",
    "type": "code",
    "section": "五、代码题",
    "title": "编写一个Shell脚本，实现备份指定目录下最近7天内修改过的.py文件到备份目录，并自动删除30天前的备份文件。",
    "code": "#!/bin/bash\n\n# 定义源目录和备份目录\nSOURCE_DIR=\"/home/user/projects\"\nBACKUP_DIR=\"/home/user/backups\"\n\n# 创建备份目录（如果不存在）\nmkdir -p \"$BACKUP_DIR\"\n\n# 备份最近 7 天内修改过的 .py 文件\nfind \"$SOURCE_DIR\" -name \"*.py\" -mtime -7 -exec cp {} \"$BACKUP_DIR\" \\;\n\n# 删除 30 天前的备份文件\nfind \"$BACKUP_DIR\" -name \"*.py\" -mtime +30 -delete\n\necho \"备份完成！\"",
    "language": "bash",
    "note": "请查资料补全代码注释"
  },
  {
    "id": "code_5",
    "type": "code",
    "section": "五、代码题",
    "title": "使用Pandas读取CSV文件，清洗数据（处理缺失值、异常值），并进行简单的统计分析。",
    "code": "import pandas as pd\nimport numpy as np\n\n# 读取 CSV 文件\ndata = pd.read_csv('data.csv')\n\n# 数据清洗：缺失值前向填充\ndata.fillna(method='ffill', inplace=True)\n\n# 异常值处理：3σ 原则\nfor col in data.select_dtypes(include=[np.number]).columns:\n    mean = data[col].mean()\n    std = data[col].std()\n    data = data[(data[col] > mean - 3 * std) & (data[col] < mean + 3 * std)]\n\n# 统计分析\nprint(\"数据基本信息：\")\nprint(data.info())\n\nprint(\"\\n描述性统计：\")\nprint(data.describe())\n\nprint(\"\\n相关性矩阵：\")\nprint(data.corr())",
    "language": "python",
    "note": "请查资料补全代码注释"
  },
  {
    "id": "code_6",
    "type": "code",
    "section": "五、代码题",
    "title": "使用PyTorch或TensorFlow实现一个简单的Transformer模型进行文本分类。",
    "code": "参考答案：\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# 加载预训练模型和分词器\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n# 示例数据\ntexts = [\"This is a positive review\", \"This product is bad\"]\nlabels = [1, 0]\n\n# 数据预处理\ninputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n\n# 模型训练（简化示例）\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\nfor epoch in range(3):\n    outputs = model(**inputs, labels=torch.tensor(labels))\n    loss = outputs.loss\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n\n# 模型预测\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predictions = torch.argmax(outputs.logits, dim=1)\n    print(f\"预测结果: {predictions}\")",
    "language": "python",
    "note": "请查资料补全代码注释"
  }
]