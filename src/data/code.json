[
  {
    "id": "code_1",
    "type": "code",
    "section": "五、代码题",
    "title": "编写脚本监控系统CPU和内存使用率，超过阈值时发送警告。",
    "code": "#!/bin/bash\n#定义阈值\nCPU_THRESHOLD=80\nMEMORY_THRESHOLD=90\n#获取CPU使用率（去掉百分号）\nCPU_USAGE=$(top -bn1 | grep \"Cpu(s)\" | awk '{print $2}' | cut -d'%' -f1 | cut -d'.' -f1)\n#获取内存使用率\nMEMORY_USAGE=$(free | grep Mem | awk '{printf \"%.0f\", $3/$2 * 100}')\n#检查CPU使用率\nif [ $CPU_USAGE -gt $CPU_THRESHOLD ]; then\necho \"警告: CPU使用率 ${CPU_USAGE}% 超过阈值 ${CPU_THRESHOLD}%\" | mail -s \"系统监控警告\" admin@example.com\nfi\n#检查内存使用率\nif [ $MEMORY_USAGE -gt $MEMORY_THRESHOLD ]; then\necho \"警告: 内存使用率 ${MEMORY_USAGE}% 超过阈值 ${MEMORY_THRESHOLD}%\" | mail -s \"系统监控警告\" admin@example.com\nfi\necho \"监控完成: CPU=${CPU_USAGE}%, 内存=${MEMORY_USAGE}%\"",
    "language": "bash",
    "note": "请查资料补全代码注释"
  },
  {
    "id": "code_2",
    "type": "code",
    "section": "五、代码题",
    "title": "使用生成器实现斐波那契数列，并演示惰性求值优势。",
    "code": "def fibonacci_generator():\n\"\"\"生成器实现斐波那契数列\"\"\"\na, b = 0, 1\nwhile True:\nyield a\na, b = b, a + b\n#演示惰性求值优势\ndef demo_lazy_evaluation():\n# 创建生成器\nfib_gen = fibonacci_generator()\n# 只获取前20个斐波那契数，不会计算整个无限序列\nfirst_20 = [next(fib_gen) for _ in range(20)]\nprint(\"前20个斐波那契数:\", first_20)\n# 内存效率演示：生成器不存储所有值\nimport sys\nfib_gen2 = fibonacci_generator()\nsize_small = sys.getsizeof(fib_gen2)\n# 对比列表存储所有值的内存占用\nfib_list = [next(fibonacci_generator()) for _ in range(1000)]\nsize_large = sys.getsizeof(fib_list)\nprint(f\"生成器内存占用: {size_small} bytes\")\nprint(f\"列表内存占用: {size_large} bytes\")\nprint(f\"内存节省: {size_large - size_small} bytes\")\nif name == \"main\":\ndemo_lazy_evaluation()",
    "language": "python",
    "note": "请查资料补全代码注释"
  },
  {
    "id": "code_3",
    "type": "code",
    "section": "五、代码题",
    "title": "使用Hugging Face Transformers库实现文本摘要任务。",
    "code": "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\nclass TextSummarizer:\ndef init(self, model_name=\"facebook/bart-large-cnn\"):\n\"\"\"初始化文本摘要器\"\"\"\nself.tokenizer = AutoTokenizer.from_pretrained(model_name)\nself.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n# 使用pipeline简化操作\nself.summarizer = pipeline(\"summarization\",\nmodel=model_name,\ntokenizer=model_name)\ndef summarize(self, text, max_length=150, min_length=50):\n\"\"\"生成文本摘要\"\"\"\n# 预处理文本\nif len(text.split()) < 50:\nreturn \"文本过短，无法生成有效摘要\"\n# 生成摘要\nsummary = self.summarizer(text,\nmax_length=max_length,\nmin_length=min_length,\ndo_sample=False)\nreturn summary[0]['summary_text']\ndef evaluate_summary(self, original_text, summary):\n\"\"\"评估摘要质量\"\"\"\n# 计算压缩比\norig_words = len(original_text.split())\nsumm_words = len(summary.split())\ncompression_ratio = (orig_words - summ_words) / orig_words * 100\nprint(f\"原文长度: {orig_words} 词\")\nprint(f\"摘要长度: {summ_words} 词\")\nprint(f\"压缩率: {compression_ratio:.1f}%\")\n# 简单的内容保留评估\nimportant_words = set(original_text.lower().split()[:10])  # 取前10个词作为重要词\npreserved_words = important_words.intersection(set(summary.lower().split()))\npreservation_rate = len(preserved_words) / len(important_words) * 100\nprint(f\"重要词保留率: {preservation_rate:.1f}%\")\n# 使用示例\nif name == \"main\":\n# 创建摘要器\nsummarizer = TextSummarizer()\n# 示例文本\nsample_text = \"\"\"\n人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器。\n该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。人工智能从诞生以来，理论和技术日益成熟，\n应用领域也不断扩大，可以设想，未来人工智能带来的科技产品，将会是人类智慧的容器。人工智能可以对人的意识、思维的信息过程的模拟。\n人工智能不是人的智能，但能像人那样思考，也可能超过人的智能。\n\"\"\" # 生成摘要\nsummary = summarizer.summarize(sample_text)\nprint(\"生成的摘要:\", summary)\nprint(\"\\n\" + \"=\"*50)\n#评估摘要质量\nsummarizer.evaluate_summary(sample_text, summary)",
    "language": "python",
    "note": "请查资料补全代码注释"
  },
  {
    "id": "code_4",
    "type": "code",
    "section": "五、代码题",
    "title": "编写一个Shell脚本，实现备份指定目录下最近7天内修改过的.py文件到备份目录，并自动删除30天前的备份文件。",
    "code": "#!/bin/bash\n#定义源目录和备份目录\nSOURCE_DIR=\"/home/user/projects\"\nBACKUP_DIR=\"/home/user/backups\"\n#创建备份目录（如果不存在）\nmkdir -p $BACKUP_DIR\n#备份最近7天内修改过的.py文件\nfind $SOURCE_DIR -name \"*.py\" -mtime -7 -exec cp {} $BACKUP_DIR ;\n#删除30天前的备份文件\nfind $BACKUP_DIR -name \"*.py\" -mtime +30 -delete\necho \"备份完成！\"",
    "language": "bash",
    "note": "请查资料补全代码注释"
  },
  {
    "id": "code_5",
    "type": "code",
    "section": "五、代码题",
    "title": "使用Pandas读取CSV文件，清洗数据（处理缺失值、异常值），并进行简单的统计分析。",
    "code": "import pandas as pd\nimport numpy as np\n#读取CSV文件\ndata = pd.read_csv('data.csv')\n#数据清洗：处理缺失值\ndata.fillna(method='ffill', inplace=True)  # 前向填充\n#处理异常值：使用3σ原则\nfor col in data.select_dtypes(include=[np.number]).columns:\nmean = data[col].mean()\nstd = data[col].std()\ndata = data[(data[col] > mean - 3std) & (data[col] < mean + 3std)]\n#统计分析\nprint(\"数据基本信息：\")\nprint(data.info())\nprint(\"\\n描述性统计：\")\nprint(data.describe())\nprint(\"\\n相关性矩阵：\")\nprint(data.corr())",
    "language": "python",
    "note": "请查资料补全代码注释"
  },
  {
    "id": "code_6",
    "type": "code",
    "section": "五、代码题",
    "title": "使用PyTorch或TensorFlow实现一个简单的Transformer模型进行文本分类。",
    "code": "参考答案：\nimport torch\nimport torch.nn as nn\nfrom transformers import BertTokenizer, BertForSequenceClassification\n#加载预训练模型和分词器\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n#示例数据\ntexts = [\"This is a positive review\", \"This product is bad\"]\nlabels = [1, 0]\n#数据预处理\ninputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n#模型训练（简化示例）\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nfor epoch in range(3):\noutputs = model(**inputs, labels=torch.tensor(labels))\nloss = outputs.loss\nloss.backward()\noptimizer.step()\noptimizer.zero_grad()\nprint(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n#模型预测\nwith torch.no_grad():\noutputs = model(**inputs)\npredictions = torch.argmax(outputs.logits, dim=1)\nprint(f\"预测结果: {predictions}\")",
    "language": "python",
    "note": "请查资料补全代码注释"
  }
]